@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={1998},
  publisher={MIT press}
}

@article{botvinick2009hierarchically,
  title={Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective},
  author={Botvinick, Matthew M and Niv, Yael and Barto, Andrew C},
  journal={Cognition},
  volume={113},
  number={3},
  pages={262--280},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{li2006towards,
  title={Towards a Unified Theory of State Abstraction for MDPs.},
  author={Li, Lihong and Walsh, Thomas J and Littman, Michael L},
  booktitle={ISAIM},
  year={2006}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@incollection{diuk2013divide,
  title={Divide and conquer: hierarchical reinforcement learning and task decomposition in humans},
  author={Diuk, Carlos and Schapiro, Anna and C{\'o}rdova, Natalia and Ribas-Fernandes, Jos{\'e} and Niv, Yael and Botvinick, Matthew},
  booktitle={Computational and robotic models of the hierarchical organization of behavior},
  pages={271--291},
  year={2013},
  publisher={Springer}
}

@article {Botvinick2014Model,
	author = {Botvinick, Matthew and Weinstein, Ari},
	title = {Model-based hierarchical reinforcement learning and human action control},
	volume = {369},
	number = {1655},
	year = {2014},
	doi = {10.1098/rstb.2013.0480},
	publisher = {The Royal Society},
	abstract = {Recent work has reawakened interest in goal-directed or {\textquoteleft}model-based{\textquoteright} choice, where decisions are based on prospective evaluation of potential action outcomes. Concurrently, there has been growing attention to the role of hierarchy in decision-making and action control. We focus here on the intersection between these two areas of interest, considering the topic of hierarchical model-based control. To characterize this form of action control, we draw on the computational framework of hierarchical reinforcement learning, using this to interpret recent empirical findings. The resulting picture reveals how hierarchical model-based mechanisms might play a special and pivotal role in human decision-making, dramatically extending the scope and complexity of human behaviour.},
	issn = {0962-8436},
	URL = {http://rstb.royalsocietypublishing.org/content/369/1655/20130480},
	eprint = {http://rstb.royalsocietypublishing.org/content/369/1655/20130480.full.pdf},
	journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences}
}
