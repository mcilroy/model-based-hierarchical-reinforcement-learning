to do 
	-clean up code
	-model = do 50,000 iterations without a goal state reward to learn options
	fix depth not working
	figure of results of # of searchs, including model free
	figure of results of # of depths, including model free
	figure of dynamic programming for discussion
	results
	discussion
		discuss dynamic programming
		discuss why using V was wrong
		discuss why using more searches works better
		results show that training time increases, but number of iterations vastly decreases. Trade off. With larger CPU resources, model-based is preferable.
	conclusion
		future work
	







########################################
	experiment - test depth and search attemps
		for search_attempts=1:10
			for depth=1:10
				for run=1:25
					load weights from model
					ts = queue(10);
					for trial=1:1000
						for t=1:500
							for s=1:search_attempts
								for d=1:depth
									if goal_reached then break
						ts.dequeue(1)
						ts.enqueue(t)
						if the median of the last ts == the fewest possible t to reach the goal then 
							break (4 when using options, 16 when using only primitives)
					median_t = median(ts)
					values.append(median_t)
				avg_value(search_attempts,depth).values = average(values)
				avg_value(search_attempts,depth).depth = depth
				avg_value(search_attempts,depth).search_attempts = search_attempts
		save(avg_value)
	
		
			
		
			
search 0:10
	depth 0:10

when depth = 0 no lookahead
when search = 0 no lookahead

0;0 0;1 0;2
1;0 1;1 1;2
2;0 2;1 2;2

using V function with depth can lead to problems
	example: moving to the bottom rooms then the goal takes longer but treated as the same as moving through the top rooms
	solution: use discounted cumulative reward. You have an estimate, in our case we just assume the cumulative reward. But can be added to the model.
	then the the two paths will not be equal.